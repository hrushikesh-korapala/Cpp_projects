# -*- coding: utf-8 -*-
"""neural_network_multi_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iGw9w6SJpXK1x8Fi_jCYNOQ9OnlLSiHO
"""

import numpy as np
import matplotlib.pyplot as plt

class NeuralNetwork:

  def __init__(self,hidden_layer_sizes=[16,16,16,16], alpha=0.5, lam=0.15, num_iters=100 , batch_size=500):
    ''' '__init__' takes hidden_layer_sizes(as a list), alpha(learning rate), lam(lambda), num_iters(number of iterations), s1(size of hidden layer), 
    batch_size(mini batch size). All of these values have been initialized by default values, but can be changed when required.'''
    self.layers = hidden_layer_sizes
    self.alpha = alpha
    self.lam = lam
    self.num_iters = num_iters
    self.bs = batch_size

  def sigmoid(self,z):
    ''' 'sigmoid' method takes z as argument and returns sigmoid value of it.'''
    return 1/(1 + np.exp(-1*z))

  def scale(self, X):
    ''' 'scale' method takes X(array) and applies feature scaling per feature by standardizing the data.
    and return the modified array.'''
    m, n = X.shape
    for i in range(n):
      X[:,i] = (X[:,i] - np.mean(X[:,i])) / (np.std(X[:,i]) + 0.0000001)
    return X

  def add_bias(self,b):
    ''' 'add_bias' method takes b(array) and adds a coloum of ones at the start as bias
    and returns the modified array'''
    return np.hstack((np.ones((len(b),1)),b))

  def remove_bias(self,b):
    ''' 'add_bias' method takes b(array) and adds a coloum of ones at the start as bias
    and returns the modified array'''
    return b[:,1:]

  def fit(self,X,y,num_cls):
    ''' 'fit' method takes X, y(Train data) and num_cls(Number of classes/outputs)
    and updates theta(parameters/weights) by gradient descent.'''
    X = self.scale(X)    # Feature Scaling
    X = self.add_bias(X)    # Adding bias colunmn
    m, n = X.shape
    
    # Adding input and output layer dimensions
    self.layers.insert(0, n)
    self.layers.append(num_cls)
    L = len(self.layers)

    # Coverting y matrix for One vs all classification
    self.num_cls = num_cls
    y_cls = np.zeros((m,self.num_cls))
    for i in range(m):
      y_cls[i][y[i]] = 1

    # Initializing some useful variables.
    # 'J.hist' and 'iters' keeps track of cost with each iteration.
    self.J_hist = []
    self.iters = []
    iters_temp = 0
    self.theta = []

    # Initializing parameter vectors theta.
    for i in range(1,L):
      if i==1:
        self.theta.append(np.random.randn(self.layers[i],self.layers[i-1])*0.01)
      else:
        self.theta.append(np.random.randn(self.layers[i],self.layers[i-1]+1)*np.sqrt(2/self.layers[i-1])) # h-et-al initialization

    for i in range(self.num_iters):
       for k in range(m//self.bs):
         X_mini = X[k*self.bs:(k+1)*self.bs,:]
         y_mini = y_cls[k*self.bs:(k+1)*self.bs,:]
         #Forward-Propagation
         a = []
         for j in range(L-1):
             if j==0:
                 a1 = X_mini
                 a.append(a1)
                 z2 = a1@self.theta[j].T
                 a2 = self.sigmoid(z2)
                 a2 = self.add_bias(a2)
             elif j<L-2:
                 a1 = a2
                 z2 = a1@self.theta[j].T
                 a2 = self.sigmoid(z2)
                 a2 = self.add_bias(a2)
             else:               
                 a1 = a2
                 z2 = a1@self.theta[j].T
                 a2 = self.sigmoid(z2)
             a.append(a2)
          
         # Cost Function
         cost = (-1/self.bs)*np.sum(y_mini*np.log(a[-1]) + (1-y_mini)*np.log(1-a[-1]))
         for j in range(L-1):
             cost += (self.lam/(2*self.bs))*(np.sum((self.theta[j])[:,1:]**2))      
         self.J_hist.append(cost)
         iters_temp += 1
         self.iters.append(iters_temp)

         # Back-Propagation
         delta = []
         grad = []
         for j in range(L-1,0,-1):
             if j==L-1:
                 delta2 = (a[j] - y_mini)
                 delta_temp = delta2.T@a[j-1]
             elif j==L-2:
                 delta2 = (delta1@self.theta[j])*(a[j])*(1-a[j])
                 delta_temp = delta2[:,1:].T@a[j-2]
             else:
                 delta1 = delta1[:,1:]
                 delta2 = (delta1@self.theta[j])*(a[j])*(1-a[j])
                 delta_temp = delta2[:,1:].T@a[j-1]
             
             theta_temp = (self.theta[j-1])
             theta_temp[:,0] = 0
             grad.insert(0, ((1/self.bs)*(delta_temp + self.lam*theta_temp)))
             delta1 = delta2
        
         # Parameter Update
         for j in range(L-1):
           self.theta[j] -= self.alpha*(grad[j])
              

  def predict(self,X):
    ''' 'predict' method takes X as argument and returns y_pred(Values predicted by trained model)'''
    X = self.scale(X)
    X = self.add_bias(X)
    L = len(self.layers)
    for j in range(L-1):
        if j==0:
            a1 = X
            z2 = a1@self.theta[j].T
            a2 = self.sigmoid(z2)
            a2 = self.add_bias(a2)
        elif j<L-2:
            a1 = a2
            z2 = a1@self.theta[j].T
            a2 = self.sigmoid(z2)
            a2 = self.add_bias(a2)
        else:               
            a1 = a2
            z2 = a1@self.theta[j].T
            a2 = self.sigmoid(z2)
    y_pred = np.argmax(a2, axis=1).reshape(len(a2),1)
    return y_pred    

  def plot(self):
    ''' 'plot' method plots J_hist vs iters (Cost value vs number of iterations).'''
    plt.plot(self.iters,self.J_hist)
    plt.xlabel("Number Of Iterations")
    plt.ylabel("Cost Function")
    plt.title("Cost Function vs Iteration")

  def accuracy(self,y,y_pred):
    ''' 'accuracy' method takes y(True values) and y_pred(Predicted values)
    and returns the accuracy value of trained model.'''
    return (np.mean(y==y_pred))*100

