# -*- coding: utf-8 -*-
"""neural_network_2layered.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m5Mi7eU5sWey37B2yYdBJCo_-Wk7SfRk
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


class NeuralNetwork:

    def __init__(self, alpha=0.5, lam=0.15, num_iters=1000, hidden_layer_size=40):
        ''' '__init__' takes alpha(learning rate), lam(lambda), num_iters(number of iterations) and s1(size of hidden layer).
    All of these values have been initialized by default values, but can be changed when required.'''
        self.alpha = alpha
        self.lam = lam
        self.num_iters = num_iters
        self.s1 = hidden_layer_size

    def sigmoid(self, z):
        ''' 'sigmoid' method takes z as argument and returns sigmoid value of it.'''
        return 1 / (1 + np.exp(-1 * z))

    def scale(self, X):
        ''' 'scale' method takes X(array) and applies feature scaling per feature by standardizing the data.
    and return the modified array.'''
        m, n = X.shape
        for i in range(n):
            X[:, i] = (X[:, i] - np.mean(X[:, i])) / (np.std(X[:, i]) + 0.0000001)
        return X

    def add_bias(self, b):
        ''' 'add_bias' method takes b(array) and adds a column of ones at the start as bias
    and returns the modified array'''
        return np.hstack((np.ones((len(b), 1)), b))

    def fit(self, X, y, n_cls):
        ''' 'fit' method takes X(array of features as train data), y(array of corresponding target values for train data),
    n_cls(number of different target values/classes) and trains the neural network.'''

        X = self.scale(X)  # Feature Scaling
        X = self.add_bias(X)  # Adding bias column
        m, n = X.shape

        # Converting y matrix for One vs all classification
        self.n_cls = n_cls
        y_cls = np.zeros((m, self.n_cls))
        for i in range(m):
            y_cls[i][y[i]] = 1

        # Initializing parameter vectors theta.
        self.theta1 = np.random.randn(self.s1, n) * 0.01
        self.theta2 = np.random.randn(n_cls, self.s1 + 1) * np.sqrt(2 / self.s1)  # h-et-al initialization

        # Initializing some useful variables.
        # 'J.history' and 'iters' keeps track of cost with each iteration.
        self.J_hist = []
        self.iters = []

        for i in range(self.num_iters):
            # Forward Propagation
            a1 = X
            z2 = a1 @ self.theta1.T
            a2 = self.sigmoid(z2)
            a2 = self.add_bias(a2)
            z3 = a2 @ self.theta2.T
            a3 = self.sigmoid(z3)

            # Cost Function
            cost = (-1 / m) * np.sum(y_cls * np.log(a3) + (1 - y_cls) * np.log(1 - a3)) + (self.lam / (2 * m)) * (
                    np.sum(self.theta1[:, 1:] ** 2) + np.sum(self.theta2[:, 1:] ** 2))
            self.J_hist.append(cost)
            self.iters.append(i)

            # Back Propagation
            # Finding err(Error), delta, D(gradient).
            err3 = (a3 - y_cls)
            err2 = (err3 @ self.theta2) * a2 * (1 - a2)
            delta2 = err3.T @ a2
            delta1 = err2[:, 1:].T @ a1
            theta1_temp = self.theta1
            theta1_temp[:, 0] = 0
            theta2_temp = self.theta2
            theta2_temp[:, 0] = 0
            D2 = (1 / m) * (delta2 + self.lam * theta2_temp)
            D1 = (1 / m) * (delta1 + self.lam * theta1_temp)
            # Parameter update.
            self.theta1 -= self.alpha * (D1)
            self.theta2 -= self.alpha * (D2)

    def plot(self):
        ''' 'plot' method plots J_hist vs iters (Cost value vs number of iterations).'''
        plt.plot(self.iters, self.J_hist)
        plt.xlabel("Number Of Iterations")
        plt.ylabel("Cost Function")
        plt.title("Cost Function vs Iteration")

    def predict(self, X):
        ''' 'predict' method takes X(Array of features) and returns y_pred(Values predicted by trained model).'''
        X = self.scale(X)
        X = self.add_bias(X)
        a1 = X
        z2 = a1 @ self.theta1.T
        a2 = self.sigmoid(z2)
        a2 = self.add_bias(a2)
        z3 = a2 @ self.theta2.T
        a3 = self.sigmoid(z3)
        y_pred = np.argmax(a3, axis=1).reshape(len(a3), 1)
        return y_pred

    def accuracy(self, y, y_pred):
        ''' 'accuracy' method takes y(True values) and y_pred(Predicted values)
    and returns the accuracy value of trained model.'''
        return (np.mean(y == y_pred)) * 100
