# -*- coding: utf-8 -*-
"""neural_network_2layered_minibatch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1aJc7ric5dMhpNrNOP-mRNns1kUKotD
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class NeuralNetwork:

  def __init__(self, alpha=0.5, lam=0.15, num_iters=50, hidden_layer_size=40, batch_size=500):
    ''' '__init__' takes alpha(learning rate), lam(lambda), num_iters(number of iterations), s1(size of hidden layer), 
    batch_size(mini batch size). All of these values have been initialized by default values, but can be changed when required.'''
    self.alpha = alpha
    self.lam = lam
    self.num_iters = num_iters
    self.s1 = hidden_layer_size
    self.bs = batch_size

  def sigmoid(self,z):
    ''' 'sigmoid' method takes z as argument and returns sigmoid value of it.'''
    return 1/(1 + np.exp(-1*z))

  def scale(self, X):
    ''' 'scale' method takes X(array) and applies feature scaling per feature by standardizing the data.
    and return the modified array.'''
    m, n = X.shape
    for i in range(n):
      X[:,i] = (X[:,i] - np.mean(X[:,i])) / (np.std(X[:,i]) + 0.0000001)
    return X

  def add_bias(self,b):
    ''' 'add_bias' method takes b(array) and adds a column of ones at the start as bias
    and returns the modified array'''
    return np.hstack((np.ones((len(b),1)),b))

  def fit(self, X, y, n_cls):
    ''' 'fit' method takes X(array of features as train data), y(array of corresponding target values for train data),
    n_cls(number of different target values/classes) and trains the neural network.'''
    
    X = self.scale(X)    # Feature Scaling
    X = self.add_bias(X)    # Adding bias column
    m, n = X.shape

    # Converting y matrix for One vs all classification
    self.n_cls = n_cls
    y_cls = np.zeros((m,self.n_cls))
    for i in range(m):
      y_cls[i][y[i]] = 1

    # Initializing parameter vectors theta.
    self.theta1 = np.random.randn(self.s1,n)*0.01
    self.theta2 = np.random.randn(n_cls,self.s1+1)*np.sqrt(2/self.s1) # h-et-al initialization

    # Initializing some useful variables.
    # 'J.history' and 'iters' keeps track of cost with each iteration.
    self.J_hist = []
    self.iters = []
    n = 0

    for i in range(self.num_iters):
      # Applying FP, BP and gradient descent in mini batches of 128 samples
      for j in range(m//self.bs):
        X_mini = X[j*self.bs:(j+1)*self.bs,:]
        y_mini = y_cls[j*self.bs:(j+1)*self.bs,:]
        # Forward Propagation
        a1 = X_mini
        z2 = a1@self.theta1.T
        a2 = self.sigmoid(z2)
        a2 = self.add_bias(a2)
        z3 = a2@self.theta2.T
        a3 = self.sigmoid(z3)

        # Cost Function
        cost = (-1/self.bs)*np.sum(y_mini*np.log(a3) + (1-y_mini)*np.log(1-a3)) + (self.lam/(2*self.bs))*(np.sum(self.theta1[:,1:]**2) + np.sum(self.theta2[:,1:]**2))
        n += 1
        self.J_hist.append(cost)
        self.iters.append(n)
        
        # Back Propagation
        # Finding err(Error), delta, D(gradient).
        err3 = (a3 - y_mini)
        err2 = (err3@self.theta2)*a2*(1-a2)
        delta2 = err3.T@a2
        delta1 = err2[:,1:].T@a1
        theta1_temp = self.theta1
        theta1_temp[:,0] = 0
        theta2_temp = self.theta2
        theta2_temp[:,0] = 0
        D2 = (1/self.bs)*(delta2 + self.lam*theta2_temp)
        D1 = (1/self.bs)*(delta1 + self.lam*theta1_temp)
        # Parameter update.
        self.theta1 -= self.alpha*(D1)
        self.theta2 -= self.alpha*(D2)
      
  def plot(self):
    ''' 'plot' method plots J_hist vs iters (Cost value vs number of iterations).'''
    plt.plot(self.iters,self.J_hist)
    plt.xlabel("Number Of Iterations")
    plt.ylabel("Cost Function")
    plt.title("Cost Function vs Iteration")

  def predict(self,X):
    ''' 'predict' method takes X(Array of features) and returns y_pred(Values predicted by trained model).'''
    X = self.scale(X)
    X = self.add_bias(X)
    a1 = X
    z2 = a1@self.theta1.T
    a2 = self.sigmoid(z2)
    a2 = self.add_bias(a2)
    z3 = a2@self.theta2.T
    a3 = self.sigmoid(z3)
    y_pred = np.argmax(a3, axis=1).reshape(len(a3),1)
    return y_pred

  def accuracy(self,y,y_pred):
    ''' 'accuracy' method takes y(True values) and y_pred(Predicted values)
    and returns the accuracy value of trained model.'''
    return (np.mean(y==y_pred))*100


